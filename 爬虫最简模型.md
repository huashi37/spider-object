#爬虫工作流程
1. 下载网页，获取网页源代码
2. 解析网页
3. 利用正则表达式或xpath提取需要的信息


#爬虫代码最简模型
##1. 分析需求 
如：求职信息、表情包、租房信息
##2. 分析网页源代码，找到信息的共同点
##3. 导入相关库
##4. 写入网址
url = "……"
##5. 编写下载网页部分的代码 requests库
   
###Ⅰ. 常用用法

    import requests
    requests.get()  requests.post()

###Ⅱ. requests的返回值 最常用的是“.text”和".content"，前者输出unicode，后者输出二进制 

    requests.get().text
    requests.get().content

##6. 编写解析网页的代码 正则、xpath、BeautifulSoup4库

###Ⅰ. 正则re 最常用法

    import re
     
    re.findall('<XXX`>(.*?)<`/XXX>',网页源代码文本.text,re.S)
   主力部队，把所有满足正则的内容提取出来，用于匹配满足某个条件的大量我们需要的内容。
          
    re.findall('<XXX`>(.*?)<`/XXX>',网页源代码文本.text,re.S)  
   狙击手，用来匹配第一个找到的元素，它的目标目的就是找到我们明显知道只有一个的元素比如标题什么的，一旦找到就结束，所以它的执行速度很快。

    re.sub('<XXX`>(.*?)<`/XXX>',网页源代码文本.text,replace)
   后勤，它的功能是替换，一般用于替换一个网页地址中的关键词，替换页码等。它看似不重要，但是往往能在很多方面给我们提供便利

###Ⅱ. xpath

    from lxml import etree
    // 定位根节点
    / 往下一层路径寻找
    [@XX=xx] 特定的标签
    /text() 提取内容并以文本返回
    /@xxxx 提取属性内容
特殊用法——以相同字符开头

    string(.) 当前层的所有内容作为一个字符串输出
    start-with(@属性名称，属性字符相同部分) 
    eg：start-with（str）所有以这个str开头的标签

###Ⅲ. BeautifulSoup4

    from bs4 import BeautifulSoup
    soup.X (X为任意标签，返回整个标签，包括标签的属性，内容等）
    如：soup.title
    soup.a  （注：仅仅返回第一个结果）
    soup.find_all('a') （find_all 可以返回所有）
find还可以按属性查找
    
    soup.find(id="link3") 
要取某个标签的某个属性，可用函数有 find_all,get

    for link in soup.find_all('a'):
      print(link.get('href'))  
要取html文件中的所有文本，可使用get_text()

    print(soup.get_text())
如果是打开html文件，语句可用：

    soup = BeautifulSoup(open("index.html"))
BeautifulSoup中的Object对象 

tag （对应html中的标签） tag.attrs (以字典形式返回tag的所有属性）
   
可以直接对tag的属性进行增、删、改，跟操作字典一样

    tag['class'] = 'verybold'

    tag['id'] = 1



 
##7. 

 
